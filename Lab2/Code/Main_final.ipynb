{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from pyclustering.cluster import cluster_visualizer_multidim\n",
    "from pyclustering.utils import timedcall\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the clustering results with N (where 2<= N <= 10) quantiles of the medianHouseValue feature values in the original dataset.\n",
    "def compareWithOriginalLabels(target_label, predict):\n",
    "    print('='*10,'Compare with original labels','='*10)\n",
    "    concatDf = pd.concat([target_label, predict],axis=1)\n",
    "    print('===count===')\n",
    "    print(concatDf.groupby('predict')['median_house_value'].count())\n",
    "    print('===max===')\n",
    "    print(concatDf.groupby('predict')['median_house_value'].max())\n",
    "    print('===median===')\n",
    "    print(concatDf.groupby('predict')['median_house_value'].median())\n",
    "    print('===min===')\n",
    "    print(concatDf.groupby('predict')['median_house_value'].min())\n",
    "    print('===mean===')\n",
    "    print(concatDf.groupby('predict')['median_house_value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestParameter(n_clusters, df, method=None, metrics=['manhattan','euclidean']):\n",
    "    # Initialize variables\n",
    "    range_of_clusters = list(range(2,n_clusters+1))\n",
    "    silhouette_avg_manhattan=[]\n",
    "    silhouette_avg_euclidean=[]\n",
    "\n",
    "    # Exception handling\n",
    "    if method == None:\n",
    "        print('[Error]: No method specified')\n",
    "        return\n",
    "\n",
    "    print('Start calculating silhouette_score...( method =',method,')')\n",
    "    for metric in metrics:\n",
    "        for k in range_of_clusters:\n",
    "            # print('Calculating silhouette_score ( k =',k,')')\n",
    "\n",
    "            # method=KMeans\n",
    "            if method == 'KMeans':\n",
    "                # initialize kmeans\n",
    "                models = KMeans(n_clusters = k).fit(df)\n",
    "                labels = models.labels_\n",
    "\n",
    "                # save silhouette score\n",
    "                if metric == 'euclidean':\n",
    "                    silhouette_avg_euclidean.append(silhouette_score(df,\n",
    "                    labels, metric=metric))\n",
    "                elif metric == 'manhattan':\n",
    "                    silhouette_avg_manhattan.append(silhouette_score(df,\n",
    "                    labels, metric=metric))\n",
    "\n",
    "            # method=GMM\n",
    "            elif method == 'GMM':\n",
    "                # initialize GMM\n",
    "                labels = GaussianMixture(n_components = k).fit_predict(df)\n",
    "\n",
    "                # save silhouette score\n",
    "                if metric == 'euclidean':\n",
    "                    silhouette_avg_euclidean.append(silhouette_score(df,\n",
    "                    labels, metric=metric))\n",
    "                elif metric == 'manhattan':\n",
    "                    silhouette_avg_manhattan.append(silhouette_score(df,\n",
    "                    labels, metric=metric))\n",
    "\n",
    "    # plotting graph (manhattan)\n",
    "    plt.clf()\n",
    "    plt.plot(range_of_clusters,silhouette_avg_manhattan,'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Silhouette score (manhattan)') \n",
    "    plt.title('Silhouette analysis For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting graph (euclidean)\n",
    "    plt.clf()\n",
    "    plt.plot(range_of_clusters,silhouette_avg_euclidean,'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Silhouette score (euclidean)') \n",
    "    plt.title('Silhouette analysis For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # return 2 best score\n",
    "    return_bestK=[]\n",
    "    for i in range(2):\n",
    "        k_num = silhouette_avg_euclidean.index(max(silhouette_avg_euclidean))\n",
    "        return_bestK.append(k_num+2)\n",
    "        silhouette_avg_euclidean[k_num]=0\n",
    "    return return_bestK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoML(dataset, target_label, encoder_list=[], scaler_list=[], model_list_and_params=[]):\n",
    "    # AutoML Start\n",
    "    print('='*15,'AutoML Start','='*15)\n",
    "\n",
    "    # Encoding ---------------------------------------\n",
    "    for encoder in encoder_list:\n",
    "        print('='*15,'Encoding (',encoder,')','='*15)\n",
    "        # Initialize dataset\n",
    "        df = dataset\n",
    "\n",
    "        # Extract categorical feature\n",
    "        needEncodeArray = df[['ocean_proximity']]\n",
    "\n",
    "        # Encode\n",
    "        encoded_array = encoder.fit_transform(needEncodeArray)\n",
    "\n",
    "        # Concatenate needEncodeArray & df\n",
    "        df = df.drop(['ocean_proximity'], axis=1)\n",
    "        encoded_df = pd.concat([df,pd.DataFrame(encoded_array, columns=['ocean_proximity'])],axis=1)\n",
    "        # display(encoded_df)\n",
    "\n",
    "        # Scaling ------------------------------------\n",
    "        for scaler in scaler_list:\n",
    "            print('='*15,'Scaling (',scaler,')','='*15)\n",
    "            scaled_df = scaler.fit_transform(encoded_df)\n",
    "            scaled_df = pd.DataFrame(scaled_df,columns=encoded_df.columns)\n",
    "            # display(scaled_df)\n",
    "            # print(scaled_df.info())\n",
    "\n",
    "            # Modeling with besk k -------------------\n",
    "            for modelName in model_list_and_params:\n",
    "                # Get modelName and model_params\n",
    "                model_params = model_list_and_params.get(modelName)\n",
    "                # print(modelName)\n",
    "                # print(model_params)\n",
    "                \n",
    "                # K-means\n",
    "                if modelName == 'KMeans':\n",
    "                    print('='*15,'Model :',modelName,'='*15)\n",
    "                    # Find optimal parameter(k) using silhouette score\n",
    "                    bestK_s = findBestParameter(12,scaled_df,modelName,['manhattan','euclidean'])\n",
    "                    print('best K_s =',bestK_s)\n",
    "\n",
    "                    # Get parameters\n",
    "                    max_iters = model_params.get('max_iter')\n",
    "                    algorithms = model_params.get('algorithm')\n",
    "\n",
    "                    for bestK in bestK_s:\n",
    "                        for max_iter in max_iters:\n",
    "                            for algorithm in algorithms:\n",
    "\n",
    "                                # Make model and fit\n",
    "                                model = KMeans(n_clusters=bestK, max_iter=max_iter, algorithm=algorithm, random_state=12)\n",
    "                                predict = pd.DataFrame(model.fit_predict(scaled_df))\n",
    "                                predict.columns = ['predict']\n",
    "                                r = pd.concat([scaled_df,predict],axis=1)\n",
    "                                print('max_iter =',max_iter, '/ algorithm =',algorithm, '/ k =',bestK, \"Done.\")\n",
    "\n",
    "                                # Plotting with pairplot\n",
    "                                plt.clf()\n",
    "                                sns.pairplot(r,hue='predict')\n",
    "                                plt.show()\n",
    "\n",
    "                                # Compare predictions with origianl target_label\n",
    "                                compareWithOriginalLabels(target_label,predict)\n",
    "                \n",
    "                # GMM\n",
    "                elif modelName == 'GMM':\n",
    "                    print('='*15,'Model :',modelName,'='*15)\n",
    "                    # Find optimal parameter(k) using silhouette score\n",
    "                    bestK_s = findBestParameter(12,scaled_df,modelName,['manhattan','euclidean'])\n",
    "                    print('best K_s =',bestK_s)\n",
    "\n",
    "                    # Get parameters\n",
    "                    covariance_types = model_params.get('covariance_type')\n",
    "                    init_params = model_params.get('init_params')\n",
    "\n",
    "                    for bestK in bestK_s:\n",
    "                        for covariance_type in covariance_types:\n",
    "                            for init_param in init_params:\n",
    "\n",
    "                                # Make model and fit_predict\n",
    "                                model = GaussianMixture(n_components=bestK, covariance_type=covariance_type, init_params=init_param, random_state=12)\n",
    "                                predict = pd.DataFrame(model.fit_predict(scaled_df))\n",
    "                                predict.columns = ['predict']\n",
    "                                r = pd.concat([scaled_df,predict],axis=1)\n",
    "                                print('covariance_type =',covariance_type, '/ init_params =',init_param, '/ k =',bestK, \"Done.\")\n",
    "\n",
    "                                # Plotting with pairplot\n",
    "                                plt.clf()\n",
    "                                sns.pairplot(r,hue='predict')\n",
    "                                plt.show()\n",
    "\n",
    "                                # Compare predictions with origianl target_label\n",
    "                                compareWithOriginalLabels(target_label,predict)\n",
    "\n",
    "                # CLARANS\n",
    "                elif modelName == 'CLARANS':\n",
    "                    print('='*15,'Model :',modelName,'='*15)\n",
    "                    # Convert dataset to list\n",
    "                    scaled_df = scaled_df[:200]\n",
    "                    scaled_df_list = scaled_df.values.tolist()\n",
    "\n",
    "                    # Get parameters\n",
    "                    n_clusters = model_params.get('number_clusters')\n",
    "                    numlocals = model_params.get('numlocal')\n",
    "                    maxneighbors = model_params.get('maxneighbor')\n",
    "\n",
    "                    for clusterNum in n_clusters:\n",
    "                        for numlocal in numlocals:\n",
    "                            for maxneighbor in maxneighbors:\n",
    "\n",
    "                                print(\"CLARANS Processing...\")\n",
    "                                # Make model and fit_predict\n",
    "                                model = clarans(scaled_df_list, clusterNum, numlocal, maxneighbor)\n",
    "                                # result = model.process\n",
    "                                result = model.process()\n",
    "                                # print(\"Execution time :\",ticks)\n",
    "                                print(\"CLARANS Processed.\")\n",
    "\n",
    "                                # Get results\n",
    "                                clusters = model.get_clusters()\n",
    "                                medoids = model.get_medoids()\n",
    "\n",
    "                                # Plotting\n",
    "                                vis = cluster_visualizer_multidim();\n",
    "                                vis.append_clusters(clusters,scaled_df_list,marker=\"*\",markersize=2);\n",
    "                                vis.show();\n",
    "\n",
    "                # DBSCAN\n",
    "                elif modelName == 'DBSCAN':\n",
    "                    print('='*15,'Model :',modelName,'='*15)\n",
    "\n",
    "                    # Get parameters\n",
    "                    epss = model_params.get('eps')\n",
    "                    min_samples = model_params.get('min_samples')\n",
    "                    metrics = model_params.get('metric')\n",
    "\n",
    "                    for eps in epss:\n",
    "                        for min_sample in min_samples:\n",
    "                            for metric in metrics:\n",
    "\n",
    "                                # Make model and fit_predict\n",
    "                                model = DBSCAN(eps=eps, min_samples=min_sample, metric=metric)\n",
    "                                predict = pd.DataFrame(model.fit_predict(scaled_df))\n",
    "                                predict.columns = ['predict']\n",
    "                                r = pd.concat([scaled_df,predict],axis=1)\n",
    "                                print('min_samples =',min_sample, '/ eps =',eps, '/ metric =',metric, \"Done.\")\n",
    "\n",
    "                                # Plotting with pairplot\n",
    "                                plt.clf()\n",
    "                                sns.pairplot(r,hue='predict')\n",
    "                                plt.show()\n",
    "\n",
    "                                # Compare predictions with origianl target_label\n",
    "                                compareWithOriginalLabels(target_label,predict)\n",
    "\n",
    "                elif modelName == 'Mean shift':\n",
    "                    print('='*15,'Model :',modelName,'='*15)\n",
    "                    # Find optimal parameter(bandwidth) using estimate_bandwidth\n",
    "                    bandwidth = estimate_bandwidth(scaled_df)\n",
    "                    print('best bandwidth =',bandwidth)\n",
    "\n",
    "                    # Get parameters\n",
    "                    max_iters = model_params.get('max_iter')\n",
    "                    bandwidths = model_params.get('bandwidth')\n",
    "\n",
    "                    for max_iter in max_iters:\n",
    "                        for bandwidth in bandwidths:\n",
    "\n",
    "                            # Make model and fit_predict\n",
    "                            model = MeanShift(max_iter=max_iter, bandwidth=bandwidth)\n",
    "                            predict = pd.DataFrame(model.fit_predict(scaled_df))\n",
    "                            predict.columns = ['predict']\n",
    "                            r = pd.concat([scaled_df,predict],axis=1)\n",
    "                            print('max_iter =',max_iter, '/ bandwidth =',bandwidth, \"Done.\")\n",
    "\n",
    "                            # Plotting with pairplot\n",
    "                            plt.clf()\n",
    "                            sns.pairplot(r,hue='predict')\n",
    "                            plt.show()\n",
    "\n",
    "                            # Compare predictions with origianl target_label\n",
    "                            compareWithOriginalLabels(target_label,predict)\n",
    "                else:\n",
    "                    print('[Error]: Wrong modelName :',modelName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('housing.csv')\n",
    "\n",
    "# Data Exploration & Preprocessing -------------------\n",
    "print('='*15,'<Original Dataset>','='*15)\n",
    "print(df.info(), end='\\n\\n')\n",
    "display(df)\n",
    "\n",
    "# Drop median_house_value column\n",
    "target_label = df['median_house_value']\n",
    "df = df.drop(['median_house_value'], axis=1)\n",
    "\n",
    "# Drop dirty data in total_bedrooms\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Check modified dataset\n",
    "print('='*15,'<Modified Dataset>','='*15)\n",
    "print(df.info(), end='\\n\\n')\n",
    "display(df)\n",
    "# ----------------------------------------------------\n",
    "# Setting parameters ---------------------------------\n",
    "# encoder list\n",
    "encoder_list = [\n",
    "    preprocessing.LabelEncoder(), \n",
    "    preprocessing.OrdinalEncoder()\n",
    "]\n",
    "# scaler list\n",
    "scaler_list = [\n",
    "    preprocessing.StandardScaler(), \n",
    "    preprocessing.MinMaxScaler(), \n",
    "    preprocessing.RobustScaler(), \n",
    "    preprocessing.MaxAbsScaler(), \n",
    "    preprocessing.Normalizer()\n",
    "]\n",
    "# model list and parameters\n",
    "model_list_and_params = {\n",
    "    'KMeans':{\n",
    "        'max_iter':[100,300,500],\n",
    "        'algorithm':['full','elkan']\n",
    "    },\n",
    "    'GMM':{\n",
    "        'covariance_type':['full', 'tied', 'diag'],\n",
    "        'init_params':['kmeans', 'random']\n",
    "    },\n",
    "    'CLARANS':{\n",
    "        'number_clusters':[2],\n",
    "        'numlocal':[5],\n",
    "        'maxneighbor':[8]\n",
    "    },\n",
    "    'DBSCAN':{\n",
    "        'eps':[0.3,1,1.5],\n",
    "        'min_samples':[100,200],\n",
    "        'metric':['euclidean', 'manhattan']\n",
    "    },\n",
    "    'Mean shift':{\n",
    "        'max_iter':[100,300,500],\n",
    "        'bandwidth':[2,3,4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 1. Full dataset, full parameters\n",
    "AutoML(\n",
    "    dataset=df, \n",
    "    target_label=target_label,\n",
    "    encoder_list=encoder_list, \n",
    "    scaler_list=scaler_list,\n",
    "    model_list_and_params = model_list_and_params,\n",
    ")\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Setting parameters (reduced) -----------------------\n",
    "# encoder list\n",
    "encoder_list = [\n",
    "    preprocessing.LabelEncoder(), \n",
    "    # preprocessing.OrdinalEncoder()\n",
    "]\n",
    "# scaler list\n",
    "scaler_list = [\n",
    "    preprocessing.StandardScaler(), \n",
    "    # preprocessing.MinMaxScaler(), \n",
    "    # preprocessing.RobustScaler(), \n",
    "    # preprocessing.MaxAbsScaler(), \n",
    "    # preprocessing.Normalizer()\n",
    "]\n",
    "# model list and parameters\n",
    "model_list_and_params = {\n",
    "    'KMeans':{\n",
    "        'max_iter':[300],\n",
    "        'algorithm':['elkan']\n",
    "    },\n",
    "    'GMM':{\n",
    "        'covariance_type':['tied'],\n",
    "        'init_params':['random']\n",
    "    },\n",
    "    'CLARANS':{\n",
    "        'number_clusters':[2],\n",
    "        'numlocal':[5],\n",
    "        'maxneighbor':[8]\n",
    "    },\n",
    "    'DBSCAN':{\n",
    "        'eps':[1],\n",
    "        'min_samples':[200],\n",
    "        'metric':['euclidean']\n",
    "    },\n",
    "    'Mean shift':{\n",
    "        'max_iter':[300],\n",
    "        'bandwidth':[3]\n",
    "    }\n",
    "}\n",
    "# # 2. Full dataset, reduced parameters\n",
    "AutoML(\n",
    "    dataset=df, \n",
    "    target_label=target_label,\n",
    "    encoder_list=encoder_list, \n",
    "    scaler_list=scaler_list,\n",
    "    model_list_and_params = model_list_and_params,\n",
    ")\n",
    "\n",
    "# # Data Preprocessing ----------------------------\n",
    "# # Drop 'longitude','latitude','housing_median_age','median_income' columns\n",
    "df = df.drop(['longitude','latitude','housing_median_age','median_income'], axis=1)\n",
    "\n",
    "# Check modified dataset\n",
    "print('='*15,'<Modified Dataset>','='*15)\n",
    "print(df.info(), end='\\n\\n')\n",
    "display(df)\n",
    "\n",
    "# Setting parameters (full) -----------------------\n",
    "# encoder list\n",
    "encoder_list = [\n",
    "    preprocessing.LabelEncoder(), \n",
    "    preprocessing.OrdinalEncoder()\n",
    "]\n",
    "# scaler list\n",
    "scaler_list = [\n",
    "    preprocessing.StandardScaler(), \n",
    "    preprocessing.MinMaxScaler(), \n",
    "    preprocessing.RobustScaler(), \n",
    "    preprocessing.MaxAbsScaler(), \n",
    "    preprocessing.Normalizer()\n",
    "]\n",
    "# model list and parameters\n",
    "model_list_and_params = {\n",
    "    'KMeans':{\n",
    "        'max_iter':[100,300,500],\n",
    "        'algorithm':['full','elkan']\n",
    "    },\n",
    "    'GMM':{\n",
    "        'covariance_type':['full', 'tied', 'diag'],\n",
    "        'init_params':['kmeans', 'random']\n",
    "    },\n",
    "    'CLARANS':{\n",
    "        'number_clusters':[2],\n",
    "        'numlocal':[5],\n",
    "        'maxneighbor':[8]\n",
    "    },\n",
    "    'DBSCAN':{\n",
    "        'eps':[0.3,1,1.5],\n",
    "        'min_samples':[100,200],\n",
    "        'metric':['euclidean', 'manhattan']\n",
    "    },\n",
    "    'Mean shift':{\n",
    "        'max_iter':[100,300,500],\n",
    "        'bandwidth':[2,3,4]\n",
    "    }\n",
    "}\n",
    "# 3. Reduced dataset, full parameters\n",
    "AutoML(\n",
    "    dataset=df, \n",
    "    target_label=target_label,\n",
    "    encoder_list=encoder_list, \n",
    "    scaler_list=scaler_list,\n",
    "    model_list_and_params = model_list_and_params,\n",
    ")\n",
    "\n",
    "# Setting parameters (reduced) -----------------------\n",
    "# encoder list\n",
    "encoder_list = [\n",
    "    preprocessing.LabelEncoder(), \n",
    "    # preprocessing.OrdinalEncoder()\n",
    "]\n",
    "# scaler list\n",
    "scaler_list = [\n",
    "    preprocessing.StandardScaler(), \n",
    "    # preprocessing.MinMaxScaler(), \n",
    "    # preprocessing.RobustScaler(), \n",
    "    # preprocessing.MaxAbsScaler(), \n",
    "    # preprocessing.Normalizer()\n",
    "]\n",
    "# model list and parameters\n",
    "model_list_and_params = {\n",
    "    'KMeans':{\n",
    "        'max_iter':[300],\n",
    "        'algorithm':['elkan']\n",
    "    },\n",
    "    'GMM':{\n",
    "        'covariance_type':['tied'],\n",
    "        'init_params':['random']\n",
    "    },\n",
    "    'CLARANS':{\n",
    "        'number_clusters':[2],\n",
    "        'numlocal':[5],\n",
    "        'maxneighbor':[8]\n",
    "    },\n",
    "    'DBSCAN':{\n",
    "        'eps':[0.5],\n",
    "        'min_samples':[300],\n",
    "        'metric':['euclidean']\n",
    "    },\n",
    "    'Mean shift':{\n",
    "        'max_iter':[300],\n",
    "        'bandwidth':[2]\n",
    "    }\n",
    "}\n",
    "# 4. Reduced dataset, reduced parameters\n",
    "AutoML(\n",
    "    dataset=df, \n",
    "    target_label=target_label,\n",
    "    encoder_list=encoder_list, \n",
    "    scaler_list=scaler_list,\n",
    "    model_list_and_params = model_list_and_params\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7206f95f23aaa87d6db2be1a3befeba5fe856b7ae7a340425c2d129bf881bddc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
